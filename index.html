<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TAPIR provides fast and accurate tracking of any point in a video">
  <meta name="keywords" content="TAPIR Tracking Any Point">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://deepmind.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://arxiv.org/abs/2211.03726">
            TAP-Vid Dataset
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://www.carldoersch.com">Carl Doersch</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://yangyi02.github.io">Yi Yang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Jvi_XPAAAAAJ">Mel Vecerik</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=cnbENAEAAAAJ">Dilara Gokay</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ankushgupta.org">Ankush Gupta</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.csail.mit.edu/yusuf/">Yusuf Aytar</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=IUZ-7_cAAAAJ">Joao Carreira</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Google DeepMind,</span>
            <span class="author-block"><sup>2</sup>VGG, Department of Engineering Science, University of Oxford</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="TODO"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="TODO"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/deepmind/tapnet"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline width="70%" style="display: block; margin: 0 auto;">
        <source src="./static/videos/swaying.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">TAPIR</span> accurately tracks any desired point on a physical surface.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          We present a new model for Tracking Any Point (TAP) that effectively tracks a query point in a video sequence. Our approach employs two stages: (1) a matching stage, which independently locates a suitable candidate point match for the query point on every other frame, and (2) a refinement stage, which updates both the trajectory and query features based on local correlations. The resulting model surpasses all baseline methods by a significant margin on the <a href="https://github.com/deepmind/tapnet">TAP-Vid benchmark</a>, as demonstrated by an approximate 20% absolute average Jaccard (AJ) improvement on DAVIS. Moreover, our model facilitates fast parallel inference on long video sequences. <span class="dnerf">TAPIR</span> can also run in an online fashion, tracking 256 points on a 256x256 video at roughly 40 fps, and can be flexibly extended to higher-resolution videos.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!--<h2 class="title is-3">Demo Videos</h2>-->
        <div class="publication-video">
          <video id="teaser" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/concat.mp4"
                    type="video/mp4">
          </video>
        </div>
          <p>
            This visualization begins with dense TAPIR tracks.  We segment the scene into foreground
            and background, remove background points, and compensate for camera motion to reveal how
            the objects move through the scene.
          </p>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video Summary</h2>
        <div class="publication-video">
          <video id="summary" playsinline controls height="100%">
            <source src="./static/videos/summary_video.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>


    <div class="columns is-centered">

      <!-- Colab. -->
      <div class="column is-four-fifths">
        <div class="content">
          <h2 class="title is-3">Demos</h2>
          <p>
          TAPIR is open-source.  We provide two online Colab demos where you can try it on your own videos without installation: the first lets you <a href="https://colab.sandbox.google.com/drive/1rYwh-U4hz1KVh3vrYzAq20x4IOG__K6g">run our best-performing TAPIR model</a> and the second lets you <a href="https://colab.sandbox.google.com/drive/1z11ECNRMDNJkEACcDIoVpq7JBWBSqcU8">run a model in an online fashion</a>.  Alternatively, you can <a href="https://github.com/deepmind/tapnet">clone our codebase</a> and run TAPIR live, tracking points on your own webcam; with a modern GPU, this demo can run in real time. Have fun!
          </p>
        </div>
      </div>
    </div>
 
  </div>


  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Arch. -->
      <div class="column is-four-fifths">
        <div class="content">
          <h2 class="title is-3">Architecture</h2>
          <p>
            TAPIR begins with our prior work, <a href="https://github.com/deepmind/tapnet">TAP-Net</a> to initialize a trajectory given a query point, and then uses an architecture inspired by <a href="https://particle-video-revisited.github.io/">Persistent Independent Particles</a> (PIPs) to refine the initial estimate.
          </p>
          <img src="./static/images/tapir_figure_simplified.png"
                 class="interpolation-image"
                 alt="TAPIR architecture."/>

          <p>
            TAP-Net lets us replace the &ldquo;Chaining&rdquo;, which was the slowest part of PIPs.  We furthermore replace the MLP-Mixer with a fully-convolutional network, which allows us to remove complex chunking procedures while improving performance.  Finally, the model estimates its own uncertainty regarding position, which improves performance and can also be useful in domains like 3D reconstruction, where confident errors can break downstream algorithms.
          </p>
        </div>
      </div>
    </div>
    <div class="columns is-centered">

      <!-- Results. -->
      <div class="column is-four-fifths">
        <div class="content">
          <h2 class="title is-3">TAP-Vid Performance</h2>
          <p>
            The <a href="https://github.com/deepmind/tapnet">TAP-Vid benchmark</a> is a set of real and synthetic videos annotated with point tracks.  The metric, Average Jaccard, measures both accuracy in estimating position and occlusion.  Higher is better.
          </p>
          <table>
            <tr>
              <th><p style="text-align:left">Method</p></th>
              <th><p style="text-align:center">Kinetics</p></th>
              <th><p style="text-align:center">DAVIS</p></th>
              <th><p style="text-align:center">Kubric</p></th>
              <th><p style="text-align:center">RGB-Stacking</p></th>
            </tr>
            <tr>
              <td style="border-bottom: none;"><p style="text-align:left">TAP-Net</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">46.6</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">38.4</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">65.4</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">59.9</p></td>
            </tr>
            <tr>
              <td style="border-bottom: none;"><p style="text-align:left">PIPs</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">35.3</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">42.0</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">59.1</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">37.3</p></td>
            </tr>
            <tr>
              <td style="border-bottom: none;"><p style="text-align:left">TAPIR</p></td>
              <td style="border-bottom: none;"><p style="text-align:center"><b>60.2</b></p></td>
              <td style="border-bottom: none;"><p style="text-align:center"><b>62.9</b></p></td>
              <td style="border-bottom: none;"><p style="text-align:center"><b>88.3</b></p></td>
              <td style="border-bottom: none;"><p style="text-align:center"><b>73.3</b></p></td>
            </tr>
 
          </table>
          <p>
          You can see that TAPIR provides a substantial boost in performance, roughly 20% absolute performance over prior methods.  To get a sense of how much this is, here's a few examples of our improvements over prior work on the DAVIS dataset
          <!--if the occlusion predictions are all correct, then 20% corresponds to roughly halving the localization error (in terms of distance between the ground truth and predicted positions) of <i>all</i> predictions.-->
          </p>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <video poster="" id="camel" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/camel.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item">
                <video poster="" id="bmx-trees" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/bmx-trees.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item">
                <video poster="" id="india" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/india.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item">
                <video poster="" id=car-shadow"" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/car-shadow.mp4"
                          type="video/mp4">
                </video>
              </div>
            </div>
          </div>


        </div>
      </div>

    </div>

    <!--/ Matting. -->

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            Point tracking is a new field with a few notable works released around the same time as ours.
          </p>
          <p>
            <a href="https://particle-video-revisited.github.io/">Persistent Independent Particles</a> was an inspiration for this work, and we'd like to thank Adam Harley for insightful discusisons.
          </p>
          <p>
            <a href="https://omnimotion.github.io/">Tracking Everything Everywhere All At Once</a> doesn't perform as well as TAPIR and is substantially slower, but it provides pseudo-3D reconstructions, and could potentially be used on top of TAPIR tracks to further improve performance.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2305.12998">Multi-Flow Tracking</a> hypothesizes many flow fields between different pairs of frames and scores them; multiple hypotheses leads to improved robustness.
          </p>
        </div>
      </div>
    </div>
    <!-- Concurrent Work. -->

  </div>
</section>
<!--

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>
-->

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="TODO">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/deepmind/tapnet" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/deepmind-tapir/deepmind-tapir.github.io">source code</a> of this website, which itelf is a fork of <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
            We just ask that you link back to this page in the footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
