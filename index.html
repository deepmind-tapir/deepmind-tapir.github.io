<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TAPIR provides fast and accurate tracking of any point in a video">
  <meta name="keywords" content="TAPIR Tracking Any Point">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="set_source(0);">

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://deepmind.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://tapvid.github.io/">
            TAP-Vid Dataset
          </a>
          <a class="navbar-item" href="https://robotap.github.io/">
            RoboTAP
          </a>
          <a class="navbar-item" href="https://deepmind-tapir.github.io/blogpost.html">
            TAPIR Blog Post
          </a>
          <a class="navbar-item" href="https://bootstap.github.io/">
            BootsTAP
          </a>
          <a class="navbar-item" href="https://tapvid3d.github.io/">
            TAPVid-3D
          </a>
	  <a class="navbar-item" href="https://tap-next.github.io/">
            TAPNext
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-2 publication-title">TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://www.carldoersch.com">Carl Doersch</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://yangyi02.github.io">Yi Yang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Jvi_XPAAAAAJ">Mel Vecerik</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=cnbENAEAAAAJ">Dilara Gokay</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ankushgupta.org">Ankush Gupta</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.csail.mit.edu/yusuf/">Yusuf Aytar</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=IUZ-7_cAAAAJ">Joao Carreira</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Google DeepMind,</span>
            <span class="author-block"><sup>2</sup>University of Oxford</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Doersch_TAPIR_Tracking_Any_Point_with_Per-Frame_Initialization_and_Temporal_Refinement_ICCV_2023_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2306.08637"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/deepmind/tapnet"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
		<span class="link-block">
		  <a href="https://github.com/google-deepmind/tapnet?tab=readme-ov-file#colab-demo" class="external-link button is-normal is-rounded is-dark">
		    <span class="icon">
		      <img src="https://colab.research.google.com/img/colab_favicon_256px.png" 
			   alt="Colab" width="16" height="16">
		    </span>
		    <span>Colab</span>
		  </a>
		</span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline width="70%" style="display: block; margin: 0 auto;">
        <source src="./static/videos/swaying.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">TAPIR</span> accurately tracks any desired point on a physical surface.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-3">Abstract</h3>
        <div class="content has-text-justified">
          <p>
          We present a new model for Tracking Any Point (TAP) that effectively tracks a query point in a video sequence. Our approach employs two stages: (1) a matching stage, which independently locates a suitable candidate point match for the query point on every other frame, and (2) a refinement stage, which updates both the trajectory and query features based on local correlations. The resulting model surpasses all baseline methods by a significant margin on the <a href="https://github.com/deepmind/tapnet">TAP-Vid benchmark</a>, as demonstrated by an approximate 20% absolute average Jaccard (AJ) improvement on DAVIS. Moreover, our model facilitates fast parallel inference on long video sequences. <span class="dnerf">TAPIR</span> can also run in an online fashion, tracking 256 points on a 256x256 video at roughly 40 fps, and can be flexibly extended to higher-resolution videos.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!--<h3 class="title is-3">Demo Videos</h3>-->
        <div class="publication-video">
          <video id="teaser" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/concat.mp4"
                    type="video/mp4">
          </video>
        </div>
          <p>
            This visualization begins with dense TAPIR tracks.  We segment the scene into foreground
            and background, remove background points, and compensate for camera motion to reveal how
            the objects move through the scene.
          </p>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-3">Video Summary</h3>
        <div class="publication-video">
          <video id="summary" playsinline controls height="100%">
            <source src="https://storage.googleapis.com/dm-tapnet/tapir_iccv_video2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>


    <div class="columns is-centered has-text-centered">

      <!-- Colab. -->
      <div class="column is-four-fifths">
        <div class="content">
          <h3 class="title is-3">Demos</h3>
          <p>
          TAPIR is open-source.  We provide two online Colab demos where you can try it on your own videos without installation: the first lets you <a href="https://colab.sandbox.google.com/github/deepmind/tapnet/blob/master/colabs/tapir_demo.ipynb">run our best-performing TAPIR model</a> and the second lets you <a href="https://colab.sandbox.google.com/github/deepmind/tapnet/blob/master/colabs/causal_tapir_demo.ipynb">run a model in an online fashion</a>.  Alternatively, you can <a href="https://github.com/deepmind/tapnet">clone our codebase</a> and run TAPIR live, tracking points on your own webcam; with a modern GPU, this demo can run in real time. Have fun!
          </p>
        </div>
      </div>
    </div>
 
  </div>


  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">

      <!-- Arch. -->
      <div class="column is-four-fifths">
        <div class="content">
          <h3 class="title is-3">Architecture</h3>
          <p>
            TAPIR begins with our prior work, <a href="https://github.com/deepmind/tapnet">TAP-Net</a> to initialize a trajectory given a query point, and then uses an architecture inspired by <a href="https://particle-video-revisited.github.io/">Persistent Independent Particles</a> (PIPs) to refine the initial estimate.
          </p>
          <img src="./static/images/tapir_figure_simplified.png"
                 class="interpolation-image"
                 alt="TAPIR architecture."/>

          <p>
            TAP-Net lets us replace the &ldquo;Chaining&rdquo;, which was the slowest part of PIPs.  We furthermore replace the MLP-Mixer with a fully-convolutional network, which allows us to remove complex chunking procedures while improving performance.  Finally, the model estimates its own uncertainty regarding position, which improves performance and can also be useful in domains like 3D reconstruction, where confident errors can break downstream algorithms.
          </p>
        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">

      <!-- Results. -->
      <div class="column is-four-fifths">
        <div class="content">
          <h3 class="title is-3">TAP-Vid Performance</h3>
          <p>
            The <a href="https://github.com/deepmind/tapnet">TAP-Vid benchmark</a> is a set of real and synthetic videos annotated with point tracks.  The metric, Average Jaccard, measures both accuracy in estimating position and occlusion.  Higher is better.
          </p>
          <table>
            <tr>
              <th><p style="text-align:left">Method</p></th>
              <th><p style="text-align:center">Kinetics</p></th>
              <th><p style="text-align:center">DAVIS</p></th>
              <th><p style="text-align:center">Kubric</p></th>
              <th><p style="text-align:center">RGB-Stacking</p></th>
            </tr>
            <tr>
              <td style="border-bottom: none;"><p style="text-align:left">TAP-Net</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">46.6</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">38.4</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">65.4</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">59.9</p></td>
            </tr>
            <tr>
              <td style="border-bottom: none;"><p style="text-align:left">PIPs</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">35.3</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">42.0</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">59.1</p></td>
              <td style="border-bottom: none;"><p style="text-align:center">37.3</p></td>
            </tr>
            <tr>
              <td style="border-bottom: none;"><p style="text-align:left">TAPIR</p></td>
              <td style="border-bottom: none;"><p style="text-align:center"><b>60.2</b></p></td>
              <td style="border-bottom: none;"><p style="text-align:center"><b>62.9</b></p></td>
              <td style="border-bottom: none;"><p style="text-align:center"><b>88.3</b></p></td>
              <td style="border-bottom: none;"><p style="text-align:center"><b>73.3</b></p></td>
            </tr>
 
          </table>
          <p>
          You can see that TAPIR provides a substantial boost in performance, roughly 20% absolute performance over prior methods.  To get a sense of how much this is, here's a few examples of our improvements over prior work on the DAVIS dataset
          <!--if the occlusion predictions are all correct, then 20% corresponds to roughly halving the localization error (in terms of distance between the ground truth and predicted positions) of <i>all</i> predictions.-->
          </p>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <video poster="" id="camel" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/camel.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item">
                <video poster="" id="bmx-trees" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/bmx-trees.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item">
                <video poster="" id="india" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/india.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item">
                <video poster="" id=car-shadow"" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/car-shadow.mp4"
                          type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <!-- Results. -->
        <div class="column is-four-fifths">
          <div class="">
   
            <h3 class="title is-3">Still Image Animation</h3>
            <p>
            One potential application of point tracking is to improve the temporal consistency and the physical plausibility of video generation.  In this proof-of-concept, we build a pipeline which can take a still image and produce a short animated clip.  This is a two-stage pipeline, where we first have a diffusion model which produces dense tracks given the input image, and a second stage which produces a video given the input image and the trajectories.  TAPIR produces training data for both stages from otherwise unlabeled videos.  See our paper for details. <br/><br/>
            In the visualization below, we start with a single example, and generate two <i>different</i> animations from it, demonstrating that our model can understands that a single image is ambiguous.   The first column shows the input image. The second column shows a visualization of the trajectories themselves on top of the input image: purples show tracks with little motion, whereas yellows show the tracks with the most motion. The third column animates the original image according to the trajectories using standard image warping. The fourth column shows the result after filling the holes with the second diffusion model. Note that the hole filling wasn't the focus of our work; thus, unlike most concurrent work on video generation, we don't do any pre-training on images, resulting in imperfect results. We encourage you to consider whether the trajectories themselves are reasonable predictions of the future.<br/><br/>
      You can use the gallery at the bottom to navigate between examples.
            </p>
          <div class="is-centered" style="border-bottom: 1px solid darkgray; border-top: 1px solid darkgray; margin-top:10px;">
            <div style="margin:auto">
	      <table style="width:1024;padding-bottom:0px" cellpadding="0" cellspacing="0"><tr>
                  <td style="width:224;text-align:center;padding-bottom:0"><h4 style="margin-bottom:0px">&nbsp;<br/>Input: single image</h4></td>
                              <td style="width:224;text-align:center;padding-bottom:0"><h4 style="margin-bottom:0px">Trajectories computed <br/> from single image</h4></td>
                              <td style="width:224;text-align:center;padding-bottom:0"><h4 style="margin-bottom:0px">Input image warped <br/> using trajectories</h4></td>
                              <td style="width:224;text-align:center;padding-bottom:0"><h4 style="margin-bottom:0px">Animation result after <br/> hole filling</h4></td>
	      </tr>
              <tr>
                <td style="padding-left:4px;padding-right:4px;"><div style="display:inline;"><img id="input" src="" width="224" height="224"/></div></td>
                <td style="padding-left:4px;padding-right:4px;"><div style="display:inline;">
                  <video id="vid0" width="224" height="224" autoplay loop muted>
                    <source src="" type="video/mp4" />
                  </video>
                </div></td>
                <td style="padding-left:4px;padding-right:4px;"><div style="display:inline;">
                  <video id="vid1" width="224" height="224" autoplay loop muted>
                    <source src="" type="video/mp4" />
                  </video>
                </div></td>
                <td style="padding-left:4px;padding-right:4px;"><div style="display:inline;">
                  <video id="vid2" width="224" height="224" autoplay loop muted>
                    <source src="" type="video/mp4" />
                  </video>
                </div></td>
              </tr>
              <tr>
                <td style="padding-left:4px;padding-right:4px;"><div style="display:inline;"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mP8/x8AAwMCAO+ip1sAAAAASUVORK5CYII=" width="224" height="224"/></div></td>
                <td style="padding-left:4px;padding-right:4px;"><div style="display:inline;">
                  <video id="vid3" width="224" height="224" autoplay loop muted>
                    <source src="" type="video/mp4" />
                  </video>
                </div></td>
                <td style="padding-left:4px;padding-right:4px;"><div style="display:inline;">
                  <video id="vid4" width="224" height="224" autoplay loop muted>
                    <source src="" type="video/mp4" />
                  </video>
                </div></td>
                <td style="padding-left:4px;padding-right:4px;"><div style="display:inline;">
                  <video id="vid5" width="224" height="224" autoplay loop muted>
                    <source src="" type="video/mp4" />
                  </video>
                </div></td>
                </tr>
              </table>
            </div>

            <p id="text">Javascript required.</p>
            <br/>
            <h3 style="border-bottom: 1px solid darkgray;">Gallery</h3>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/image_35.png" width="54" height="54" onclick="set_source(0);"/>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/image_64.png" width="54" height="54" onclick="set_source(1);"/>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/image_43.png" width="54" height="54" onclick="set_source(2);"/>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/image_26.png" width="54" height="54" onclick="set_source(3);"/>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/image_65.png" width="54" height="54" onclick="set_source(4);"/>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/image_135.png" width="54" height="54" onclick="set_source(5);"/>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/normal-cat.png" width="54" height="54" onclick="set_source(6);"/>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/brown-bird.png" width="54" height="54" onclick="set_source(7);"/>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/image_8.png" width="54" height="54" onclick="set_source(8);"/>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/image_18.png" width="54" height="54" onclick="set_source(9);"/>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/image_25.png" width="54" height="54" onclick="set_source(10);"/>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/image_63.png" width="54" height="54" onclick="set_source(11);"/>
            <img id="input" style="margin-top:10px;cursor:pointer;" src="https://storage.googleapis.com/dm-tapnet/tapir-blogpost/videos/animation/portrait.png" width="54" height="54" onclick="set_source(12);"/>
          </div>
        </div>
      </div>
    </div>
  </div>
  </section>

  <section class="hero">
    <div class="container is-max-desktop">

    <!-- Concurrent Work. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h3 class="title is-3" style="margin-top: 20px">Related Links</h3>

        <div class="content has-text-justified">
          <p>
            Point tracking is a new field with a few notable works released around the same time as ours.
          </p>
          <p>
            <a href="https://robotap.github.io/">RoboTAP</a> is a DeepMind work applies point tracking for robotics, specifically extremely low-shot imitation of complex robot behaviors from video.
          </p>
          <p>
            <a href="https://particle-video-revisited.github.io/">Persistent Independent Particles</a> was an inspiration for this work, and we'd like to thank Adam Harley for insightful discusisons.
          </p>
          <p>
            <a href="https://omnimotion.github.io/">Tracking Everything Everywhere All At Once</a> doesn't perform as well as TAPIR and is substantially slower, but it provides pseudo-3D reconstructions, and could potentially be used on top of TAPIR tracks to further improve performance.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2305.12998">Multi-Flow Tracking</a> hypothesizes many flow fields between different pairs of frames and scores them; multiple hypotheses leads to improved robustness.
          </p>
        </div>
      </div>
    </div>
    <!-- Concurrent Work. -->

</section>
<!--

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h3 class="title">BibTeX</h3>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>
-->

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2306.08637">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/deepmind/tapnet" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/deepmind-tapir/deepmind-tapir.github.io">source code</a> of this website, which itelf is a fork of <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
            We just ask that you link back to this page in the footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
